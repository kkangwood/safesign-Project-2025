# src/rag_pipeline.py
import json
import re
import os
from dotenv import load_dotenv

# [Import]
from toxic_detector import ToxicClauseDetector
from llm_service import LLM_gemini
from law.legal_context import LawContextManager
from law.precedent_context import PrecedentContextManager

load_dotenv()

class RagPipeline:
    """
    RAG íŒŒì´í”„ë¼ì¸: ìœ í•´ì„± ê²€ì‚¬ -> ê²€ìƒ‰ -> ë‹µë³€ ìƒì„± -> Faithfulness ê²€ì¦ ë£¨í”„
    """
    def __init__(self):
        print("âš™ï¸ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        
        api_key = os.getenv("GEMINI_API_KEY")
        self.llm = LLM_gemini(gemini_api_key=api_key, model="gemini-1.5-flash")
        
        # Detector ì¬ì‚¬ìš© (ì´ë¯¸ DB ë§¤ë‹ˆì €ë¥¼ ê°€ì§€ê³  ìˆìŒ)
        self.toxic_detector = ToxicClauseDetector()
        
        # DB ë§¤ë‹ˆì € ì ‘ê·¼ì„ ìœ„í•´ Detector ë‚´ë¶€ ê°ì²´ ì°¸ì¡°
        self.law_manager = self.toxic_detector.law_manager
        self.precedent_manager = self.toxic_detector.precedent_manager

        self.MAX_RETRIES = 2
        self.TARGET_SCORE = 75

    def run(self, user_query: str):
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë²•ë¥ ì  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸
        """
        print(f"\nğŸš€ [Pipeline Start] ì§ˆë¬¸: {user_query}")

        # 1. ê²€ìƒ‰ (Retrieval)
        law_docs = self.law_manager.search_relevant_laws(user_query, k=2)
        prec_docs = self.precedent_manager.search_relevant_precedents(user_query, k=2)

        if not law_docs and not prec_docs:
            return {
                "answer": "ê´€ë ¨ëœ ë²•ë ¹ì´ë‚˜ íŒë¡€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "score": 0
            }
        
        context_text = self._format_context(law_docs, prec_docs)

        # 2. ìƒì„± ë° ê²€ì¦ ë£¨í”„ (Generation & Loop)
        current_answer = ""
        current_score = 0
        retry_count = 0
        feedback = ""

        while retry_count <= self.MAX_RETRIES:
            print(f"ğŸ“ [Attempt {retry_count + 1}] ë‹µë³€ ìƒì„± ì¤‘...")
            
            # (1) ë‹µë³€ ìƒì„±
            current_answer = self._generate_answer(user_query, context_text, feedback)

            # (2) Faithfulness í‰ê°€
            eval_result = self._evaluate_faithfulness(user_query, current_answer, context_text)
            current_score = eval_result.get('score', 0)
            reason = eval_result.get('reason', 'í‰ê°€ ë¶ˆê°€')
            
            print(f"   ğŸ‘‰ ì ìˆ˜: {current_score}ì  | ì´ìœ : {reason}")

            if current_score >= self.TARGET_SCORE:
                break
            else:
                feedback = f"ì ìˆ˜ ë¯¸ë‹¬({current_score}ì ). ì´ìœ : {reason}. ê·¼ê±° ìë£Œì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”."
                retry_count += 1

        final_sources = law_docs + prec_docs
        
        if current_score < self.TARGET_SCORE:
            current_answer = f"[ì£¼ì˜: ê·¼ê±° ë¶ˆì¶©ë¶„ (ì‹ ë¢°ë„: {current_score}%)]\n{current_answer}"

        return {
            "answer": current_answer,
            "sources": final_sources,
            "score": current_score
        }

    def _format_context(self, laws, precedents):
        formatted = ""
        if laws:
            formatted += "=== [ê´€ë ¨ ë²•ë ¹] ===\n" + "\n".join([f"{i+1}. {txt}" for i, txt in enumerate(laws)]) + "\n\n"
        if precedents:
            formatted += "=== [ê´€ë ¨ íŒë¡€] ===\n" + "\n".join([f"{i+1}. {txt}" for i, txt in enumerate(precedents)])
        return formatted

    def _generate_answer(self, query, context, feedback=""):
        system_role = "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  AIì…ë‹ˆë‹¤. ë°˜ë“œì‹œ [ì°¸ê³  ìë£Œ]ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."
        prompt = f"{system_role}\n\n[ì°¸ê³  ìë£Œ]\n{context}\n\n[ì§ˆë¬¸]\n{query}"
        if feedback:
            prompt += f"\n\n[ìˆ˜ì • ì§€ì‹œ]\n{feedback}"
        
        response = self.llm.generate(prompt)
        return response.text

    def _evaluate_faithfulness(self, query, answer, context):
        prompt = f"""
        ë‹¹ì‹ ì€ Fact Checkerì…ë‹ˆë‹¤. [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ [AI ë‹µë³€]ì´ ì‚¬ì‹¤ì— ë¶€í•©í•˜ëŠ”ì§€ 0~100ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
        ê²°ê³¼ëŠ” JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”: {{"score": 85, "reason": "..."}}

        [ì°¸ê³  ìë£Œ]
        {context}
        [AI ë‹µë³€]
        {answer}
        """
        try:
            response = self.llm.generate(prompt)
            clean_json = re.sub(r'```json|```', '', response.text).strip()
            # ì¤‘ê´„í˜¸ ì¶”ì¶œ
            start = clean_json.find('{')
            end = clean_json.rfind('}') + 1
            return json.loads(clean_json[start:end])
        except:
            return {"score": 50, "reason": "Evaluation Error"}