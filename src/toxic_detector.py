import os
from dotenv import load_dotenv
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.models.base_model import DeepEvalBaseLLM
from deepeval.metrics.g_eval import Rubric

# [Import]
from llm_service import LLM_gemini
from law.legal_context import LawContextManager
from law.precedent_context import PrecedentContextManager

#load_dotenv()

# --- 1. DeepEvalìš© Gemini ì–´ëŒ‘í„° ---
class GeminiDeepEvalAdapter(DeepEvalBaseLLM):
    def __init__(self, llm_service: LLM_gemini):
        self.llm_service = llm_service
        self.model_name = llm_service.model_name

    def load_model(self):
        return self.llm_service.client

    def generate(self, prompt: str) -> str:
        response = self.llm_service.generate(prompt)
        return response.text

    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)

    def get_model_name(self):
        return self.model_name

# --- 2. ë…ì†Œì¡°í•­ íŒë³„ê¸° í´ë˜ìŠ¤ ---
class ToxicClauseDetector:
    def __init__(self, api_key=None):
        print("ğŸ›¡ï¸ ToxicClauseDetector (Pro Model) ì´ˆê¸°í™” ì¤‘...")
        
        if not api_key:
            api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("Gemini API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")

        
        self.llm_service = LLM_gemini(gemini_api_key=api_key, model="gemini-2.5-flash")
        self.evaluator_llm = GeminiDeepEvalAdapter(self.llm_service)
        
        # DB ë§¤ë‹ˆì €
        self.law_manager = LawContextManager()
        self.precedent_manager = PrecedentContextManager()
        self.law_manager.initialize_database()
        self.precedent_manager.initialize_database()

        # [í•µì‹¬ ë³€ê²½ 2] í”„ë¡¬í”„íŠ¸ ì „ëµ ë³€ê²½: "ì•ˆì „ ì ìˆ˜" -> "ìœ„í—˜ ì ìˆ˜(Toxicity Score)"
        # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ 'ìœ„í—˜(Toxic)'í•œ ê²ƒìœ¼ë¡œ ê¸°ì¤€ì„ ë’¤ì§‘ìŠµë‹ˆë‹¤.
        self.toxic_criteria = """
        ë‹¹ì‹ ì€ ê·¼ë¡œì ë³´í˜¸ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•˜ëŠ” 'ê·¼ë¡œê³„ì•½ì„œ ê°ì‚¬ê´€'ì…ë‹ˆë‹¤.
        ì…ë ¥ëœ ì¡°í•­ì´ [ê´€ë ¨ ë²•ë ¹/íŒë¡€]ë¥¼ ìœ„ë°˜í•˜ê±°ë‚˜, ê·¼ë¡œìì—ê²Œ ë¶ˆê³µì •í•œ 'ë…ì†Œì¡°í•­'ì¸ì§€ í‰ê°€í•˜ì„¸ìš”.
        
        [í•„ìˆ˜ í™•ì¸ ì‚¬í•­]
        1. í‡´ì§ê¸ˆ/ì£¼íœ´ìˆ˜ë‹¹ í¬ê¸° ê°ì„œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€? (ëª…ë°±í•œ ë¶ˆë²•)
        2. 'ë¯¼í˜•ì‚¬ìƒ ì´ì˜ ì œê¸° ê¸ˆì§€' ê°™ì€ ê³¼ë„í•œ ê¶Œë¦¬ ì œí•œì´ ìˆëŠ”ê°€?
        3. ì†í•´ë°°ìƒì•¡ì„ ë¯¸ë¦¬ ì •í•´ë‘ì—ˆëŠ”ê°€? (ìœ„ì•½ ì˜ˆì • ê¸ˆì§€ ìœ„ë°˜)
        4. í•´ê³  ì˜ˆê³  ì—†ì´ ì¦‰ì‹œ í•´ê³ ê°€ ê°€ëŠ¥í•˜ë‹¤ê³  ëª…ì‹œí–ˆëŠ”ê°€?
        """

        self.rubric = [
            Rubric(score_range=(0, 2), expected_outcome="ì™„ë²½í•˜ê²Œ ê³µì •í•˜ê³  ë²•ì ì¸ ë¬¸ì œê°€ ì „í˜€ ì—†ëŠ” ì¡°í•­."),
            Rubric(score_range=(3, 5), expected_outcome="ë²•ì  íš¨ë ¥ì€ ìˆìœ¼ë‚˜ ê·¼ë¡œìì—ê²Œ ë‹¤ì†Œ ë¶ˆë¦¬í•˜ê±°ë‚˜ ëª¨í˜¸í•œ í‘œí˜„ì´ ìˆìŒ."),
            Rubric(score_range=(6, 8), expected_outcome="ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ ì†Œì§€ê°€ ë†’ê±°ë‚˜, ê·¼ë¡œìì—ê²Œ ë¶€ë‹¹í•œ ì˜ë¬´ë¥¼ ë¶€ê³¼í•¨."),
            Rubric(score_range=(9, 10), expected_outcome="ëª…ë°±í•œ ë¶ˆë²•(ìœ„ì•½ ì˜ˆì •, í‡´ì§ê¸ˆ í¬ê¸° ë“±)ì´ë©° ì ˆëŒ€ ìš©ë‚©ë  ìˆ˜ ì—†ëŠ” ë…ì†Œì¡°í•­."),
        ]

        self.evaluation_steps = [
            "ì¡°í•­ì˜ í•µì‹¬ ì˜ë„(ì„ê¸ˆ ì‚­ê°, í•´ê³  ìš©ì´ì„±, ì±…ì„ ì „ê°€ ë“±)ë¥¼ íŒŒì•…í•œë‹¤.",
            "[ê´€ë ¨ ë²•ë ¹]ì´ ì œê³µë˜ì§€ ì•Šì•˜ë”ë¼ë„, ë‹¹ì‹ ì˜ ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ì„ ë™ì›í•˜ì—¬ ìœ„ë²•ì„±ì„ íŒë‹¨í•œë‹¤.",
            "íŠ¹íˆ 'í‡´ì§ê¸ˆ í¬ê¸°', 'ì†í•´ë°°ìƒ ì˜ˆì •', 'ê°•ì œ ê·¼ë¡œ' ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ìµœê³  ìœ„í—˜ ì ìˆ˜(10ì )ë¥¼ ë¶€ì—¬í•œë‹¤.",
            "ë²•ì  ê·¼ê±°ê°€ í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•œ ìª½ìœ¼ë¡œ í•´ì„í•˜ì—¬ ì ìˆ˜ë¥¼ ë§¤ê¸´ë‹¤."
        ]

    def _retrieve_context(self, clause_text):
        # 1. ë²•ë ¹ ê²€ìƒ‰
        laws = self.law_manager.search_relevant_laws(clause_text, k=2)
        law_text = "\n".join(laws) if laws else "ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ì¼ë°˜ ë²•ë¥  ì§€ì‹ìœ¼ë¡œ íŒë‹¨ ìš”ë§)"

        # 2. íŒë¡€ ê²€ìƒ‰
        precedents = self.precedent_manager.search_relevant_precedents(clause_text, k=1)
        precedent_text = precedents[0] if precedents else "ê´€ë ¨ íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        return f"=== [ê´€ë ¨ ë²•ë ¹] ===\n{law_text}\n\n=== [ê´€ë ¨ íŒë¡€] ===\n{precedent_text}"

    def detect(self, clause_text):
        # print(f"ğŸ•µï¸ ì¡°í•­ ë¶„ì„ ì¤‘: {clause_text[:30]}...")
        
        retrieved_context = self._retrieve_context(clause_text)
        
        # G-Eval í‰ê°€
        toxic_metric = GEval(
            name="Toxicity Score", # ì´ë¦„ ë³€ê²½
            criteria=self.toxic_criteria,
            rubric=self.rubric,
            evaluation_steps=self.evaluation_steps,
            model=self.evaluator_llm, 
            threshold=5, # 5ì  ì´ìƒì´ë©´ ë…ì†Œì¡°í•­ìœ¼ë¡œ ê°„ì£¼
            evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
        )

        test_case = LLMTestCase(
            input=clause_text,
            actual_output="í‰ê°€ ëŒ€ìƒ",
            retrieval_context=[retrieved_context]
        )

        toxic_metric.measure(test_case)
        
        # [í•µì‹¬ ë³€ê²½ 3] ì ìˆ˜ í•´ì„ ë¡œì§ ë‹¨ìˆœí™”
        # ì´ì œ ì ìˆ˜(0~10)ê°€ ê³§ ìœ„í—˜ë„ì…ë‹ˆë‹¤. ë’¤ì§‘ì„ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
        risk_score = toxic_metric.score # 0~10ì  (DeepEval ë²„ì „ì— ë”°ë¼ 0~1ì¼ ìˆ˜ë„ ìˆìŒ, ì•„ë˜ ë³´ì •)
        
        # DeepEvalì´ 0~1 ì‚¬ì´ ê°’ì„ ë¦¬í„´í•˜ëŠ” ê²½ìš° 10ì„ ê³±í•´ì¤Œ
        if risk_score <= 1.0:
            risk_score *= 10
            
        # 4ì  ì´ìƒì´ë©´ ë…ì†Œì¡°í•­ (ê¸°ì¤€ ê°•í™”)
        is_toxic = risk_score >= 4.0
        
        # ë””ë²„ê¹…ìš© ì¶œë ¥ (í„°ë¯¸ë„ì—ì„œ í™•ì¸ ê°€ëŠ¥)
        print(f"[{'ğŸš¨ìœ„í—˜' if is_toxic else 'âœ…ì•ˆì „'}] ì ìˆ˜: {risk_score} | ë‚´ìš©: {clause_text[:20]}...")

        return {
            "clause": clause_text,
            "is_toxic": is_toxic,
            "risk_score": round(risk_score, 1),
            "reason": toxic_metric.reason,
            "context_used": retrieved_context
        }

    def generate_easy_suggestion(self, detection_result):
        if not detection_result['is_toxic']:
            return "âœ… **ì•ˆì „í•œ ì¡°í•­ì…ë‹ˆë‹¤.**"

        prompt = f"""
        ë‹¹ì‹ ì€ ê·¼ë¡œì í¸ì¸ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë…ì†Œì¡°í•­ì„ ë¶„ì„í•˜ì„¸ìš”.
        
        [ì›ë¬¸]: {detection_result['clause']}
        [ì´ìœ ]: {detection_result['reason']}
        [ê·¼ê±°]: {detection_result['context_used']}

        ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì‘ì„±:
        1. **âš ï¸ ì‰¬ìš´ í•´ì„**: ì´ˆë“±í•™ìƒë„ ì´í•´í•˜ê²Œ 2ë¬¸ì¥ ìš”ì•½.
        2. **ğŸ’¡ ìˆ˜ì • ì œì•ˆ**: ë²•ì— ë§ëŠ” ê³µì •í•œ ì¡°í•­ ì˜ˆì‹œ.
        """
        response = self.llm_service.generate(prompt)
        return response.text