import os
import time
from datasets import load_dataset
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings # LangChain Deprecation ê²½ê³ ë¥¼ í”¼í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì„í¬íŠ¸
from langchain_core.documents import Document

# --- ì„¤ì • ---
# â­ï¸ DB ì €ì¥ ê²½ë¡œ ë° ëª¨ë¸ ì •ì˜
DB_PATH = "../data/faiss_precedent_db" 
EMBEDDING_MODEL_NAME = "jhgan/ko-sbert-nli" 
# â­ï¸ Hugging Face ë°ì´í„°ì…‹ ID
DATASET_ID = "joonhok-exo-ai/korean_law_open_data_precedents" 
SAMPLE_SIZE = 1000 # í…ŒìŠ¤íŠ¸/êµ¬ì¶•ìš© ë°ì´í„° ê°œìˆ˜ (ì „ì²´ ì‚¬ìš© ì‹œ None)

class PrecedentContextManager:
    """
    Hugging Face ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ íŒë¡€ ë²¡í„° DBë¥¼ êµ¬ì¶•í•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    ì´ í´ë˜ìŠ¤ëŠ” DB ë¡œë“œì™€ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í†µí•© ì œê³µí•©ë‹ˆë‹¤.
    """
    def __init__(self):
        self.vectorstore = None
        # ì„ë² ë”© ëª¨ë¸ ê°ì²´ ì´ˆê¸°í™” (DB ë¡œë“œ/êµ¬ì¶• ì‹œ ì‚¬ìš©)
        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

    def _fetch_and_parse_precedents(self):
        """
        Hugging Face ë°ì´í„°ì…‹ì—ì„œ íŒë¡€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  LangChain Document ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        :return: ë³€í™˜ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“¥ íŒë¡€ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... ({DATASET_ID})")
        
        # 1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ìƒ˜í”Œë§
        try:
            dataset = load_dataset(DATASET_ID, split="train") 
            
            if SAMPLE_SIZE and len(dataset) > SAMPLE_SIZE:
                dataset = dataset.select(range(SAMPLE_SIZE)) 
                print(f"    - (ì„¤ì •) ìƒìœ„ {SAMPLE_SIZE}ê°œë§Œ ë²¡í„°í™”í•©ë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
        
        # 2. Document ê°ì²´ë¡œ ë³€í™˜
        print("ğŸ”„ ë¬¸ì„œ ê°ì²´(Document)ë¡œ ë³€í™˜ ì¤‘...")
        documents = []

        for item in dataset:
            # ë°ì´í„°ì…‹ ì»¬ëŸ¼ ë§¤í•‘ ë° ë‚´ìš© ì¶”ì¶œ
            content = item.get('ì „ë¬¸', '')
            summary = item.get('íŒê²°ìš”ì§€', '')
            case_name = item.get('ì‚¬ê±´ëª…', 'ì‚¬ê±´ëª… ì •ë³´ ì—†ìŒ')
            case_number = item.get('ì‚¬ê±´ë²ˆí˜¸', 'N/A')

            # ê²€ìƒ‰ ì •í™•ë„ë¥¼ ìœ„í•œ page_content êµ¬ì„± (íŒì‹œì‚¬í•­, ìš”ì§€ ë“± ì¤‘ìš” ì •ë³´ ê°•ì¡°)
            page_content = f"""
[ì‚¬ê±´ë²ˆí˜¸] {case_number}
[ì‚¬ê±´ëª…] {case_name}
[íŒê²°ìš”ì§€] {summary}
[ì „ë¬¸] {content[:2000]}...
""".strip()
            
            metadata = {
                "case_name": case_name, 
                "source": "HuggingFace Precedent DB",
                "case_number": case_number
            }
            
            # ìœ íš¨ì„± ê²€ì‚¬ (íŒë¡€ ìš”ì§€ê°€ ì¶©ë¶„íˆ ê¸´ ê²½ìš°ì—ë§Œ í¬í•¨)
            if len(summary) > 10: 
                 documents.append(Document(page_content=page_content, metadata=metadata))
        
        print(f"    - ë³€í™˜ëœ ìœ íš¨ ë¬¸ì„œ: {len(documents)}ê°œ")
        return documents

    def initialize_database(self):
        """
        ë¡œì»¬ DB ê²½ë¡œë¥¼ í™•ì¸í•˜ì—¬ ê¸°ì¡´ DBë¥¼ ë¡œë“œí•˜ê±°ë‚˜, ì—†ì„ ê²½ìš° ì‹ ê·œ êµ¬ì¶• í›„ ì €ì¥í•©ë‹ˆë‹¤.
        """
        if self.vectorstore is not None:
            print("ğŸ’¡ íŒë¡€ DBê°€ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return

        # 1. ë¡œì»¬ DB íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ë¡œë“œ ì‹œë„
        if os.path.exists(DB_PATH) and os.path.isdir(DB_PATH):
            print(f"âœ… [ì´ˆê¸°í™”] ê¸°ì¡´ íŒë¡€ DB ë¡œë“œ ì¤‘... (ê²½ë¡œ: {DB_PATH})")
            try:
                # FAISS ë¡œë“œ (DB êµ¬ì¶• ì‹œ ì‚¬ìš©ëœ ì„ë² ë”© ëª¨ë¸ ê°ì²´ ì „ë‹¬)
                self.vectorstore = FAISS.load_local(
                    DB_PATH, 
                    self.embeddings, 
                    allow_dangerous_deserialization=True
                )
                print(f"âœ… [ì´ˆê¸°í™”] íŒë¡€ DB ë¡œë“œ ì™„ë£Œ! (ì´ {len(self.vectorstore.docstore._dict)}ê±´)")
                return
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ DB ë¡œë“œ ì‹¤íŒ¨: {e}. DBë¥¼ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
        
        # 2. ì‹ ê·œ DB êµ¬ì¶•
        print(f"ğŸ“š [ì´ˆê¸°í™”] í•„ìˆ˜ íŒë¡€ ë°ì´í„° ì‹ ê·œ êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤. (í˜ì´ì§€ë‹¹ {display}ê±´, ìµœëŒ€ {max_pages} í˜ì´ì§€)")
        all_docs = []       # ìˆ˜ì§‘ëœ ëª¨ë“  Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        precedent_ids = set() # íŒë¡€ì¼ë ¨ë²ˆí˜¸ ì¤‘ë³µ ë°©ì§€ìš© Set

        for query in self.target_queries:
            print(f"\n  ğŸ” '{query}' ê²€ìƒ‰ ì¤‘...")
            page = 1
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¤‘ë³µ ì—†ì´ ìˆ˜ì§‘
            while page <= max_pages: 
                # íŒë¡€ ëª©ë¡ ê²€ìƒ‰
                precedents, total_count = search_precedent_list(query, display=display, page=page)
                
                if not precedents:
                    break
                
                total_pages = (total_count + display - 1) // display
                print(f"  ğŸ“¥ í˜ì´ì§€ {page}/{total_pages} ({len(precedents)}ê±´) íŒë¡€ ìƒì„¸ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±...")

                for prec_info in precedents:
                    # ==========================================
                    # [ìˆ˜ì •ë¨] ë°ì´í„° íƒ€ì… ë°©ì–´ ì½”ë“œ ì¶”ê°€ êµ¬ê°„
                    # ==========================================
                    
                    # 1. ë¬¸ìì—´(String)ì´ ì˜ëª» ë“¤ì–´ì˜¨ ê²½ìš° ì²´í¬
                    if isinstance(prec_info, str):
                        print(f"âš ï¸ [ê²½ê³ ] ì˜ˆìƒì¹˜ ëª»í•œ ë°ì´í„° íƒ€ì…(str) ë°œê²¬ -> ê±´ë„ˆëœ€. ë‚´ìš©: {prec_info}")
                        continue
                    
                    # 2. ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²´í¬
                    if not isinstance(prec_info, dict):
                        print(f"âš ï¸ [ê²½ê³ ] ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ë°ì´í„° íƒ€ì…({type(prec_info)}) ë°œê²¬ -> ê±´ë„ˆëœ€.")
                        continue

                    # 3. ì•ˆì „í•˜ê²Œ .get() í˜¸ì¶œ
                    prec_id = prec_info.get("íŒë¡€ì¼ë ¨ë²ˆí˜¸")
                    
                    if not prec_id or prec_id in precedent_ids:
                        continue 
                    
                    # 2-1. íŒë¡€ ìƒì„¸ ë‚´ìš©(ìš”ì§€, íŒì‹œì‚¬í•­) ê°€ì ¸ì˜¤ê¸°
                    summary_list, holding = get_precedent_detail_text(prec_id)
                    
                    # 2-2. ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜ ë° ì¤‘ë³µ ê²€ì‚¬ í›„ ì¶”ê°€
                    full_text, metadata = parse_precedent_content(summary_list, holding, prec_info)
                    
                    if full_text:
                        doc = Document(page_content=full_text, metadata=metadata)
                        all_docs.append(doc)
                        precedent_ids.add(prec_id) 

                page += 1
                
        if not all_docs:
            print("âŒ ì €ì¥í•  íŒë¡€ ë°ì´í„°ê°€ ì—†ì–´ DB ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # 3. ë²¡í„° DB ìƒì„± ë° ë¡œì»¬ ì €ì¥
        print(f"âš¡ ì´ {len(all_docs)}ê°œ íŒë¡€ ë²¡í„°í™” ë° DB ì €ì¥ ì‹œì‘...")
        start_time = time.time()
        
        # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ FAISS ë²¡í„° DBë¡œ ë³€í™˜ ë° ì €ì¥
        self.vectorstore = FAISS.from_documents(all_docs, self.embeddings)
        os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
        self.vectorstore.save_local(DB_PATH)
        
        elapsed_time = time.time() - start_time
        print(f"âœ… íŒë¡€ DB ì‹ ê·œ êµ¬ì¶• ë° ì €ì¥ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ, ê²½ë¡œ: {os.path.abspath(DB_PATH)})")
        
    def search_relevant_precedents(self, query, k=2):
        """
        ë¡œì»¬ì— ë¡œë“œëœ DBì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íŒë¡€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        :param query: ê²€ìƒ‰ì„ ìœ„í•œ ì‚¬ìš©ì ì§ˆë¬¸(í…ìŠ¤íŠ¸)
        :param k: ë°˜í™˜í•  ê²€ìƒ‰ ê²°ê³¼(Document)ì˜ ìµœëŒ€ ê°œìˆ˜ì…ë‹ˆë‹¤. (ê¸°ë³¸ê°’: 2)
        :return: ê²€ìƒ‰ëœ íŒë¡€ ë‚´ìš©(page_content) ë¦¬ìŠ¤íŠ¸
        """
        # DB ë¡œë“œ í™•ì¸ ë° ì‹œë„
        if not self.vectorstore:
            self.initialize_database()
        
        if not self.vectorstore:
            print("âš ï¸ íŒë¡€ DBê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ğŸ” íŒë¡€ DBì—ì„œ '{query[:20]}...' ê´€ë ¨ íŒë¡€ {k}ê°œ ê²€ìƒ‰ ì¤‘...")
        # ì¿¼ë¦¬ë¥¼ ë²¡í„°í™”í•œ í›„, DB ë‚´ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ì°¾ì•„ ì›ë¬¸ Documentë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        docs = self.vectorstore.similarity_search(query, k=k) 
        
        # Documentì˜ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
        return [doc.page_content for doc in docs]

# ==========================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ==========================================
if __name__ == "__main__":
    # DB ì €ì¥ ê²½ë¡œ ìƒì„± (ì—†ì„ ê²½ìš° ëŒ€ë¹„)
    if not os.path.exists(os.path.dirname(DB_PATH)):
        os.makedirs(os.path.dirname(DB_PATH))

    manager = PrecedentContextManager()
    
    # DB ì´ˆê¸°í™” (ë¡œë“œ ë˜ëŠ” êµ¬ì¶•)
    manager.initialize_database()
    
    # êµ¬ì¶•ëœ DBë¡œ ê²€ìƒ‰ ìˆ˜í–‰
    question = "ì§ì›ì´ ì—…ë¬´ íƒœë§Œìœ¼ë¡œ í•´ê³ ë˜ì—ˆì„ ë•Œ ë¶€ë‹¹ í•´ê³ ë¡œ ì¸ì •ë  ìˆ˜ ìˆëŠ” ê¸°ì¤€ì´ ë­ì•¼?"
    relevant_cases = manager.search_relevant_precedents(question, k=1)
    
    print("\n" + "="*50)
    print("ğŸ“ ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€:")
    print("="*50)
    
    if relevant_cases:
        print(relevant_cases[0])
    else:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")